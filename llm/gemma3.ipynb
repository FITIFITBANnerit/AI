{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "ckpt = \"google/gemma-3-1b-it\"\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt, torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = \"합격을 서울대 의대 신OO 서울대 체육교육과 박O0 축하합니다!! 서운대의대2년인아진학 학교이름은 및내쥐서 고마워요 고통화교 동창회일동\"\n",
    "no_info = \"정보를 찾을 수 없습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/banner_it/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/banner_it/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "Category: \"Other\"\n",
      "Judgment: \"illegal\"\n",
      "```\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are an expert in analyzing the purpose of text. Your task is to classify the given text into the most relevant category and determine its legality based on strict criteria.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"\n",
    "                                Analyze the meaning of the following banner text and classify it into the most relevant category from the list below.\n",
    "\n",
    "                                **Banner Text:** {ocr_text}\n",
    "\n",
    "                                ### Categories:\n",
    "                                1. Politics\n",
    "                                2. Public interest\n",
    "                                3. Commercial purposes\n",
    "                                4. Other\n",
    "\n",
    "                                ### Judgment Criteria:\n",
    "                                - If the category is Politics, Public interest then:\n",
    "                                  **Judgment: \"legal\"**\n",
    "                                \n",
    "                                - If the category is **Commercial purposes or Other**, then:\n",
    "                                  **Judgment: \"illegal\"**\n",
    "                                \n",
    "\n",
    "                                ### Output Format (Strictly Follow This):\n",
    "\n",
    "                                ```\n",
    "                                Category: \"Politics\"\n",
    "                                Judgment: \"legal\"\n",
    "                                ```\n",
    "                            \"\"\"\n",
    "                            \n",
    "                }\n",
    "            ]\n",
    "            \n",
    "        }\n",
    "    ],\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "generation = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "generation = generation[0][input_len:]\n",
    "\n",
    "decoded = tokenizer.decode(generation, skip_sepcial_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's the extraction based on the provided text:\n",
      "\n",
      "*   **Phone Number:** (Not present in the text)\n",
      "*   **Company Name:** 서운대 (Suwon Da)\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The text mentions \"서운대의대2년인아진학 학교이름은 및내쥐서\" which translates to \"The name of the school 'A Jin Hak' is and more.\"  This strongly suggests a school affiliated with Seoul National University.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are a helpful assistant who extract phone numbers and company name from text extracted from banners.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"\n",
    "                                Extract phone numbers and a company name or store name from {ocr_text}. If there are none, answer {no_info}.\n",
    "                                The output is printed as an example.\n",
    "                                \n",
    "                            \"\"\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "generation = generation[0][input_len:]\n",
    "\n",
    "decoded = tokenizer.decode(generation, skip_sepcial_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banner_it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
